# Celeryä»»åŠ¡å®šä¹‰

import time
import asyncio
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any, Optional
from celery import Celery, group, chord
from celery.exceptions import Retry
from loguru import logger
from .config import config
from .database import db_manager
from .fetcher import WebpageFetcher
from .detector import change_detector
from .notifier import notification_manager


# åˆ›å»ºCeleryåº”ç”¨
celery_app = Celery(
    'monitor',
    broker=config.celery.broker_url,
    backend=config.celery.result_backend,
    include=['monitor.tasks']
)

# Celeryé…ç½®
celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    task_track_started=True,
    worker_prefetch_multiplier=1,
    task_acks_late=True,
    worker_disable_rate_limits=False,
    task_default_retry_delay=60,
    task_max_retries=3,
    task_routes={
        'monitor.tasks.fetch_website': {'queue': 'fetch'},
        'monitor.tasks.detect_changes': {'queue': 'detect'},
        'monitor.tasks.send_notification': {'queue': 'notify'},
        'monitor.tasks.cleanup_old_data': {'queue': 'maintenance'},
        'monitor.tasks.system_health_check': {'queue': 'system'},
    },
    beat_schedule={
        'schedule-website-monitoring': {
            'task': 'monitor.tasks.schedule_website_monitoring',
            'schedule': 300,  # 5åˆ†é’Ÿ
        },
        'cleanup-old-data': {
            'task': 'monitor.tasks.cleanup_old_data',
            'schedule': 3600,  # 1å°æ—¶
        },
        'system-health-check': {
            'task': 'monitor.tasks.system_health_check',
            'schedule': 600,  # 10åˆ†é’Ÿ
        },
    }
)


@celery_app.task(bind=True, name='monitor.tasks.fetch_website')
def fetch_website(self, website_id: int) -> Dict[str, Any]:
    """æŠ“å–ç½‘ç«™å†…å®¹ä»»åŠ¡"""
    try:
        logger.info(f"å¼€å§‹æŠ“å–ç½‘ç«™ä»»åŠ¡ - ID: {website_id}, Task: {self.request.id}")
        
        # è®°å½•ä»»åŠ¡å¼€å§‹
        db_manager.log_task_start(self.request.id, 'fetch_website', {'website_id': website_id})
        
        # æ‰§è¡ŒæŠ“å–
        fetcher = WebpageFetcher()
        result = fetcher.fetch_website(website_id)
        
        # è®°å½•ä»»åŠ¡å®Œæˆ
        db_manager.log_task_completion(
            self.request.id, 
            'fetch_website', 
            result['success'], 
            result.get('error')
        )
        
        if result['success']:
            logger.info(f"ç½‘ç«™æŠ“å–ä»»åŠ¡å®Œæˆ - ID: {website_id}, Content ID: {result.get('content_id')}")
            
            # å¦‚æœæŠ“å–æˆåŠŸï¼Œè§¦å‘å˜åŒ–æ£€æµ‹
            detect_changes.delay(website_id, result.get('content_id'))
        else:
            logger.warning(f"ç½‘ç«™æŠ“å–ä»»åŠ¡å¤±è´¥ - ID: {website_id}, é”™è¯¯: {result.get('error')}")
        
        return result
        
    except Exception as e:
        logger.error(f"ç½‘ç«™æŠ“å–ä»»åŠ¡å¼‚å¸¸ - ID: {website_id}, é”™è¯¯: {str(e)}")
        
        # è®°å½•ä»»åŠ¡å¤±è´¥
        db_manager.log_task_completion(
            self.request.id, 
            'fetch_website', 
            False, 
            str(e)
        )
        
        # é‡è¯•æœºåˆ¶
        if self.request.retries < self.max_retries:
            logger.info(f"é‡è¯•ç½‘ç«™æŠ“å–ä»»åŠ¡ - ID: {website_id}, é‡è¯•æ¬¡æ•°: {self.request.retries + 1}")
            raise self.retry(countdown=60 * (self.request.retries + 1), exc=e)
        
        return {
            'success': False,
            'error': str(e),
            'website_id': website_id
        }


@celery_app.task(bind=True, name='monitor.tasks.detect_changes')
def detect_changes(self, website_id: int, content_id: Optional[int] = None) -> Dict[str, Any]:
    """æ£€æµ‹ç½‘ç«™å˜åŒ–ä»»åŠ¡"""
    try:
        logger.info(f"å¼€å§‹å˜åŒ–æ£€æµ‹ä»»åŠ¡ - ç½‘ç«™ID: {website_id}, å†…å®¹ID: {content_id}, Task: {self.request.id}")
        
        # è®°å½•ä»»åŠ¡å¼€å§‹
        db_manager.log_task_start(
            self.request.id, 
            'detect_changes', 
            {'website_id': website_id, 'content_id': content_id}
        )
        
        # æ‰§è¡Œå˜åŒ–æ£€æµ‹
        result = change_detector.detect_website_changes(website_id)
        
        if result is None:
            logger.info(f"å˜åŒ–æ£€æµ‹è·³è¿‡ - ç½‘ç«™ID: {website_id}")
            db_manager.log_task_completion(self.request.id, 'detect_changes', True, 'æ£€æµ‹è·³è¿‡')
            return {'success': True, 'has_change': False, 'reason': 'æ£€æµ‹è·³è¿‡'}
        
        # è®°å½•ä»»åŠ¡å®Œæˆ
        db_manager.log_task_completion(
            self.request.id, 
            'detect_changes', 
            True, 
            None
        )
        
        if result.has_change:
            logger.info(f"æ£€æµ‹åˆ°å˜åŒ– - ç½‘ç«™ID: {website_id}, å˜åŒ–åˆ†æ•°: {result.change_score:.3f}")
            
            # å¦‚æœæ£€æµ‹åˆ°å˜åŒ–ï¼Œå‘é€é€šçŸ¥
            send_notification.delay(website_id, result.__dict__)
        else:
            logger.info(f"æœªæ£€æµ‹åˆ°å˜åŒ– - ç½‘ç«™ID: {website_id}")
        
        return {
            'success': True,
            'has_change': result.has_change,
            'change_score': result.change_score,
            'change_type': result.change_type,
            'website_id': website_id
        }
        
    except Exception as e:
        logger.error(f"å˜åŒ–æ£€æµ‹ä»»åŠ¡å¼‚å¸¸ - ç½‘ç«™ID: {website_id}, é”™è¯¯: {str(e)}")
        
        # è®°å½•ä»»åŠ¡å¤±è´¥
        db_manager.log_task_completion(
            self.request.id, 
            'detect_changes', 
            False, 
            str(e)
        )
        
        return {
            'success': False,
            'error': str(e),
            'website_id': website_id
        }


@celery_app.task(bind=True, name='monitor.tasks.send_notification')
def send_notification(self, website_id: int, change_result_dict: Dict[str, Any]) -> Dict[str, Any]:
    """å‘é€é€šçŸ¥ä»»åŠ¡"""
    try:
        logger.info(f"å¼€å§‹å‘é€é€šçŸ¥ä»»åŠ¡ - ç½‘ç«™ID: {website_id}, Task: {self.request.id}")
        
        # è®°å½•ä»»åŠ¡å¼€å§‹
        db_manager.log_task_start(
            self.request.id, 
            'send_notification', 
            {'website_id': website_id}
        )
        
        # é‡æ„ChangeResultå¯¹è±¡
        from .detector import ChangeResult
        change_result = ChangeResult(**change_result_dict)
        
        # å‘é€é€šçŸ¥
        success = notification_manager.send_change_notification(website_id, change_result)
        
        # è®°å½•ä»»åŠ¡å®Œæˆ
        db_manager.log_task_completion(
            self.request.id, 
            'send_notification', 
            success, 
            None if success else 'é€šçŸ¥å‘é€å¤±è´¥'
        )
        
        if success:
            logger.info(f"é€šçŸ¥å‘é€æˆåŠŸ - ç½‘ç«™ID: {website_id}")
        else:
            logger.warning(f"é€šçŸ¥å‘é€å¤±è´¥ - ç½‘ç«™ID: {website_id}")
        
        return {
            'success': success,
            'website_id': website_id
        }
        
    except Exception as e:
        logger.error(f"å‘é€é€šçŸ¥ä»»åŠ¡å¼‚å¸¸ - ç½‘ç«™ID: {website_id}, é”™è¯¯: {str(e)}")
        
        # è®°å½•ä»»åŠ¡å¤±è´¥
        db_manager.log_task_completion(
            self.request.id, 
            'send_notification', 
            False, 
            str(e)
        )
        
        return {
            'success': False,
            'error': str(e),
            'website_id': website_id
        }


@celery_app.task(name='monitor.tasks.schedule_website_monitoring')
def schedule_website_monitoring() -> Dict[str, Any]:
    """è°ƒåº¦ç½‘ç«™ç›‘æ§ä»»åŠ¡"""
    try:
        logger.info("å¼€å§‹è°ƒåº¦ç½‘ç«™ç›‘æ§ä»»åŠ¡")
        
        # è·å–éœ€è¦ç›‘æ§çš„ç½‘ç«™
        websites = db_manager.get_websites_for_monitoring()
        
        if not websites:
            logger.info("æ²¡æœ‰éœ€è¦ç›‘æ§çš„ç½‘ç«™")
            return {'success': True, 'scheduled_count': 0}
        
        # æŒ‰ä¼˜å…ˆçº§å’Œæ£€æŸ¥é—´éš”åˆ†ç»„
        high_priority_sites = []
        normal_priority_sites = []
        low_priority_sites = []
        
        current_time = datetime.now(timezone.utc)
        
        for website in websites:
            # æ£€æŸ¥æ˜¯å¦åˆ°äº†æ£€æŸ¥æ—¶é—´
            if website.last_check_at:
                time_since_check = current_time - website.last_check_at
                if time_since_check.total_seconds() < website.check_interval * 60:
                    continue
            
            # æ ¹æ®ä¼˜å…ˆçº§åˆ†ç»„
            if website.priority == 'high':
                high_priority_sites.append(website.id)
            elif website.priority == 'low':
                low_priority_sites.append(website.id)
            else:
                normal_priority_sites.append(website.id)
        
        # åˆ›å»ºä»»åŠ¡ç»„
        scheduled_count = 0
        
        # é«˜ä¼˜å…ˆçº§ç½‘ç«™ç«‹å³æ‰§è¡Œ
        if high_priority_sites:
            high_priority_group = group(fetch_website.s(site_id) for site_id in high_priority_sites)
            high_priority_group.apply_async()
            scheduled_count += len(high_priority_sites)
            logger.info(f"è°ƒåº¦é«˜ä¼˜å…ˆçº§ç½‘ç«™ç›‘æ§ä»»åŠ¡: {len(high_priority_sites)} ä¸ª")
        
        # æ™®é€šä¼˜å…ˆçº§ç½‘ç«™
        if normal_priority_sites:
            normal_priority_group = group(fetch_website.s(site_id) for site_id in normal_priority_sites)
            normal_priority_group.apply_async(countdown=30)  # å»¶è¿Ÿ30ç§’
            scheduled_count += len(normal_priority_sites)
            logger.info(f"è°ƒåº¦æ™®é€šä¼˜å…ˆçº§ç½‘ç«™ç›‘æ§ä»»åŠ¡: {len(normal_priority_sites)} ä¸ª")
        
        # ä½ä¼˜å…ˆçº§ç½‘ç«™
        if low_priority_sites:
            low_priority_group = group(fetch_website.s(site_id) for site_id in low_priority_sites)
            low_priority_group.apply_async(countdown=60)  # å»¶è¿Ÿ60ç§’
            scheduled_count += len(low_priority_sites)
            logger.info(f"è°ƒåº¦ä½ä¼˜å…ˆçº§ç½‘ç«™ç›‘æ§ä»»åŠ¡: {len(low_priority_sites)} ä¸ª")
        
        logger.info(f"ç½‘ç«™ç›‘æ§ä»»åŠ¡è°ƒåº¦å®Œæˆ - æ€»è®¡: {scheduled_count} ä¸ª")
        
        return {
            'success': True,
            'scheduled_count': scheduled_count,
            'high_priority': len(high_priority_sites),
            'normal_priority': len(normal_priority_sites),
            'low_priority': len(low_priority_sites)
        }
        
    except Exception as e:
        logger.error(f"è°ƒåº¦ç½‘ç«™ç›‘æ§ä»»åŠ¡å¼‚å¸¸: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'scheduled_count': 0
        }


@celery_app.task(name='monitor.tasks.batch_fetch_websites')
def batch_fetch_websites(website_ids: List[int]) -> Dict[str, Any]:
    """æ‰¹é‡æŠ“å–ç½‘ç«™ä»»åŠ¡"""
    try:
        logger.info(f"å¼€å§‹æ‰¹é‡æŠ“å–ç½‘ç«™ä»»åŠ¡ - æ•°é‡: {len(website_ids)}")
        
        # åˆ›å»ºä»»åŠ¡ç»„
        job = group(fetch_website.s(website_id) for website_id in website_ids)
        result = job.apply_async()
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = result.get(timeout=config.celery.task_time_limit)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r.get('success', False))
        failed_count = len(results) - success_count
        
        logger.info(f"æ‰¹é‡æŠ“å–ç½‘ç«™ä»»åŠ¡å®Œæˆ - æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}")
        
        return {
            'success': True,
            'total_count': len(website_ids),
            'success_count': success_count,
            'failed_count': failed_count,
            'results': results
        }
        
    except Exception as e:
        logger.error(f"æ‰¹é‡æŠ“å–ç½‘ç«™ä»»åŠ¡å¼‚å¸¸: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'total_count': len(website_ids)
        }


@celery_app.task(name='monitor.tasks.cleanup_old_data')
def cleanup_old_data() -> Dict[str, Any]:
    """æ¸…ç†æ—§æ•°æ®ä»»åŠ¡"""
    try:
        logger.info("å¼€å§‹æ¸…ç†æ—§æ•°æ®ä»»åŠ¡")
        
        # æ¸…ç†æ—§çš„ç½‘é¡µå†…å®¹
        content_retention_days = config.storage.content_retention_days
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=content_retention_days)
        
        deleted_contents = db_manager.cleanup_old_contents(cutoff_date)
        logger.info(f"æ¸…ç†æ—§ç½‘é¡µå†…å®¹: {deleted_contents} æ¡")
        
        # æ¸…ç†æ—§çš„å˜åŒ–æ£€æµ‹è®°å½•
        detection_retention_days = config.storage.detection_retention_days
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=detection_retention_days)
        
        deleted_detections = db_manager.cleanup_old_detections(cutoff_date)
        logger.info(f"æ¸…ç†æ—§å˜åŒ–æ£€æµ‹è®°å½•: {deleted_detections} æ¡")
        
        # æ¸…ç†æ—§çš„ä»»åŠ¡æ—¥å¿—
        log_retention_days = config.storage.log_retention_days
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=log_retention_days)
        
        deleted_logs = db_manager.cleanup_old_task_logs(cutoff_date)
        logger.info(f"æ¸…ç†æ—§ä»»åŠ¡æ—¥å¿—: {deleted_logs} æ¡")
        
        # æ¸…ç†Redisç¼“å­˜
        cache_cleaned = db_manager.cleanup_expired_cache()
        logger.info(f"æ¸…ç†è¿‡æœŸç¼“å­˜: {cache_cleaned} ä¸ªé”®")
        
        total_cleaned = deleted_contents + deleted_detections + deleted_logs + cache_cleaned
        
        logger.info(f"æ•°æ®æ¸…ç†ä»»åŠ¡å®Œæˆ - æ€»è®¡æ¸…ç†: {total_cleaned} æ¡è®°å½•")
        
        return {
            'success': True,
            'deleted_contents': deleted_contents,
            'deleted_detections': deleted_detections,
            'deleted_logs': deleted_logs,
            'cache_cleaned': cache_cleaned,
            'total_cleaned': total_cleaned
        }
        
    except Exception as e:
        logger.error(f"æ¸…ç†æ—§æ•°æ®ä»»åŠ¡å¼‚å¸¸: {str(e)}")
        return {
            'success': False,
            'error': str(e)
        }


@celery_app.task(name='monitor.tasks.system_health_check')
def system_health_check() -> Dict[str, Any]:
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥ä»»åŠ¡"""
    try:
        logger.info("å¼€å§‹ç³»ç»Ÿå¥åº·æ£€æŸ¥ä»»åŠ¡")
        
        health_status = {
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'database': False,
            'redis': False,
            'celery_workers': 0,
            'active_websites': 0,
            'recent_errors': 0,
            'disk_usage': 0.0,
            'memory_usage': 0.0
        }
        
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        try:
            db_manager.test_connection()
            health_status['database'] = True
        except Exception as e:
            logger.error(f"æ•°æ®åº“è¿æ¥æ£€æŸ¥å¤±è´¥: {str(e)}")
        
        # æ£€æŸ¥Redisè¿æ¥
        try:
            db_manager.test_redis_connection()
            health_status['redis'] = True
        except Exception as e:
            logger.error(f"Redisè¿æ¥æ£€æŸ¥å¤±è´¥: {str(e)}")
        
        # æ£€æŸ¥Celeryå·¥ä½œè¿›ç¨‹
        try:
            inspect = celery_app.control.inspect()
            stats = inspect.stats()
            health_status['celery_workers'] = len(stats) if stats else 0
        except Exception as e:
            logger.error(f"Celeryå·¥ä½œè¿›ç¨‹æ£€æŸ¥å¤±è´¥: {str(e)}")
        
        # æ£€æŸ¥æ´»è·ƒç½‘ç«™æ•°é‡
        try:
            health_status['active_websites'] = db_manager.get_active_websites_count()
        except Exception as e:
            logger.error(f"æ´»è·ƒç½‘ç«™æ•°é‡æ£€æŸ¥å¤±è´¥: {str(e)}")
        
        # æ£€æŸ¥æœ€è¿‘é”™è¯¯æ•°é‡
        try:
            health_status['recent_errors'] = db_manager.get_recent_errors_count(hours=24)
        except Exception as e:
            logger.error(f"æœ€è¿‘é”™è¯¯æ•°é‡æ£€æŸ¥å¤±è´¥: {str(e)}")
        
        # æ£€æŸ¥ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
        try:
            import psutil
            health_status['disk_usage'] = psutil.disk_usage('/').percent
            health_status['memory_usage'] = psutil.virtual_memory().percent
        except Exception as e:
            logger.warning(f"ç³»ç»Ÿèµ„æºæ£€æŸ¥å¤±è´¥: {str(e)}")
        
        # ä¿å­˜å¥åº·æ£€æŸ¥ç»“æœ
        db_manager.save_system_metrics(health_status)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‘é€è­¦æŠ¥
        alerts = []
        
        if not health_status['database']:
            alerts.append('æ•°æ®åº“è¿æ¥å¤±è´¥')
        
        if not health_status['redis']:
            alerts.append('Redisè¿æ¥å¤±è´¥')
        
        if health_status['celery_workers'] == 0:
            alerts.append('æ²¡æœ‰å¯ç”¨çš„Celeryå·¥ä½œè¿›ç¨‹')
        
        if health_status['disk_usage'] > 90:
            alerts.append(f'ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜: {health_status["disk_usage"]:.1f}%')
        
        if health_status['memory_usage'] > 90:
            alerts.append(f'å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {health_status["memory_usage"]:.1f}%')
        
        if health_status['recent_errors'] > 100:
            alerts.append(f'æœ€è¿‘24å°æ—¶é”™è¯¯æ•°é‡è¿‡å¤š: {health_status["recent_errors"]}')
        
        # å‘é€ç³»ç»Ÿè­¦æŠ¥
        if alerts:
            alert_content = '\n'.join(alerts)
            notification_manager.send_system_notification(
                'ğŸš¨ ç³»ç»Ÿå¥åº·æ£€æŸ¥è­¦æŠ¥',
                f'æ£€æµ‹åˆ°ä»¥ä¸‹é—®é¢˜:\n{alert_content}',
                'urgent'
            )
            logger.warning(f"ç³»ç»Ÿå¥åº·æ£€æŸ¥å‘ç°é—®é¢˜: {alert_content}")
        
        logger.info("ç³»ç»Ÿå¥åº·æ£€æŸ¥ä»»åŠ¡å®Œæˆ")
        
        return {
            'success': True,
            'health_status': health_status,
            'alerts': alerts
        }
        
    except Exception as e:
        logger.error(f"ç³»ç»Ÿå¥åº·æ£€æŸ¥ä»»åŠ¡å¼‚å¸¸: {str(e)}")
        return {
            'success': False,
            'error': str(e)
        }


@celery_app.task(name='monitor.tasks.manual_check_website')
def manual_check_website(website_id: int) -> Dict[str, Any]:
    """æ‰‹åŠ¨æ£€æŸ¥ç½‘ç«™ä»»åŠ¡"""
    try:
        logger.info(f"å¼€å§‹æ‰‹åŠ¨æ£€æŸ¥ç½‘ç«™ä»»åŠ¡ - ID: {website_id}")
        
        # åˆ›å»ºä»»åŠ¡é“¾ï¼šæŠ“å– -> æ£€æµ‹ -> é€šçŸ¥
        from celery import chain
        
        workflow = chain(
            fetch_website.s(website_id),
            detect_changes.s(website_id)
        )
        
        result = workflow.apply_async()
        
        logger.info(f"æ‰‹åŠ¨æ£€æŸ¥ç½‘ç«™ä»»åŠ¡å·²æäº¤ - ID: {website_id}, Task Chain: {result.id}")
        
        return {
            'success': True,
            'website_id': website_id,
            'task_id': result.id
        }
        
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨æ£€æŸ¥ç½‘ç«™ä»»åŠ¡å¼‚å¸¸ - ID: {website_id}, é”™è¯¯: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'website_id': website_id
        }


# ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢å‡½æ•°
def get_task_status(task_id: str) -> Dict[str, Any]:
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    try:
        result = celery_app.AsyncResult(task_id)
        
        return {
            'task_id': task_id,
            'status': result.status,
            'result': result.result,
            'traceback': result.traceback,
            'date_done': result.date_done.isoformat() if result.date_done else None
        }
        
    except Exception as e:
        return {
            'task_id': task_id,
            'status': 'ERROR',
            'error': str(e)
        }


# é˜Ÿåˆ—çŠ¶æ€ç¼“å­˜
_queue_status_cache = {
    'data': None,
    'last_update': 0,
    'cache_ttl': 10  # ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆç§’ï¼‰
}

# é˜Ÿåˆ—çŠ¶æ€æŸ¥è¯¢å‡½æ•°
def get_queue_status() -> Dict[str, Any]:
    """è·å–é˜Ÿåˆ—çŠ¶æ€ï¼ˆå¸¦ç¼“å­˜æœºåˆ¶ï¼‰"""
    global _queue_status_cache
    
    current_time = time.time()
    
    # å¦‚æœç¼“å­˜æœ‰æ•ˆï¼Œç›´æ¥è¿”å›ç¼“å­˜æ•°æ®
    if (_queue_status_cache['data'] is not None and 
        current_time - _queue_status_cache['last_update'] < _queue_status_cache['cache_ttl']):
        return _queue_status_cache['data']
    
    try:
        inspect = celery_app.control.inspect()
        
        # è·å–æ´»è·ƒä»»åŠ¡
        active_tasks = inspect.active()
        
        # è·å–é¢„å®šä»»åŠ¡
        scheduled_tasks = inspect.scheduled()
        
        # è·å–ä¿ç•™ä»»åŠ¡
        reserved_tasks = inspect.reserved()
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = inspect.stats()
        
        result = {
            'active_tasks': active_tasks,
            'scheduled_tasks': scheduled_tasks,
            'reserved_tasks': reserved_tasks,
            'worker_stats': stats
        }
        
        # æ›´æ–°ç¼“å­˜
        _queue_status_cache['data'] = result
        _queue_status_cache['last_update'] = current_time
        
        return result
        
    except Exception as e:
        logger.error(f"è·å–é˜Ÿåˆ—çŠ¶æ€å¤±è´¥: {str(e)}")
        return {
            'error': str(e)
        }